---
title: "Exploring Covid 19 data - Part 1"
date: 2020-04-26
tags: [data science, web scraping]
excerpt: "Data Science, Web Scraping"
---

We are facing a hurtful pandemic at a global scale, and in Brazil it's not different. The numbers of death and confirmed cases grow each day and at the same time we see opinions - from the government - trying to minimize the problem.

Motivated by this [tweet](https://twitter.com/blqueiroz/status/1253090230187548675) from Professor Bernardo Queiroz, where he states the lack of detailed data to analyze mortality ratios provided by the Ministry of Health of Brazil. This institution only provides aggregate data in a special [website]() dedicated to Covid-19.

Other source of data is the administrative records of deaths, through death certificates. This data is provided by Brazilian Civil Registry Offices at ['Portal da Transparência - Registro Civil'](https://transparencia.registrocivil.org.br/especial-covid). This data is desaggregated by gender, age groups and brazilian states. So, we decided to collect it.

We need to point out the differences between the date:

1) more details about the data and divergences...

IMPORTANTE:

https://transparencia.registrocivil.org.br/especial-covid
"Ao selecionar o filtro tipo "Data de" com a opção óbito, sugerimos que escolha uma data de 10 dias antes da data atual. Dados mais recentes podem sofrer atualização devido a entrada de novos registros e correções de registros"

# Webscraping

So, how to collect the data if the website doesn't provide it directly via a .csv download, for exemple? We are stating here only examples of programmatic ways of getting data, because you can always copy and paste, manually.

During the [Summer School](http://evcomp.dcc.ufmg.br/) hosted by the [Computer Science Department](http://www.dcc.ufmg.br/dcc/) at UFMG, Bernardo Abreu and Vinícius Silva - from the company 'Big Data' - lectured a course on 'Collecting data from WEB'. They talked about getting data from an API and by 'Web scraping'.

The [presentation](http://evcomp.dcc.ufmg.br/wp-content/uploads/BigData-Coleta-de-dados-na-Web.pdf) has been shared and so the code - using an [API](https://colab.research.google.com/drive/1YnuhEgvSAkoonflKjM8w5-sZKJwS-mDn) or [Scraping HTML](https://colab.research.google.com/drive/1WBAyw2OQnKkrgi2xU5iT73eyW9aM_uC3).

The code and the examples shared by them helped to get what we needed to do. The data that we want to collect it's in this graph:

![alt]({{ site.url }}{{ site.baseurl }}/assets/images/graph_portal_transparencia.jpg)

A web page is constructed using the **HTML language** (and other programming languages, as well). Here's what a web page looks like 'behind the scenes':



What we have to know is that a web page is constructed by blocks and in the language the blocks have an hierarchy structure that helps us to find the information we want to collect. So, in the example above, we go 'div' them 'div' them 'tag' them 'h3' and we got our needed information.

But in the website we're looking at the data it's not **in** the HTML page, as you can see in the image below:

One thing that we 
