---
title: "Exploring Covid 19 data - Part 1"
date: 2020-04-26
tags: [data science, web scraping]
excerpt: "Data Science, Web Scraping"
gallery:
  - image_path: /assets/images/quotes_to_scrap.jpg
    alt: "placeholder image 1"
    title: "Quotes to scrap"
  - image_path: /assets/images/quotes_to_scrap_devtools.jpg
    alt: "placeholder image 2"
    title: "Quotes to scrap - HTML"
---

We are facing a hurtful pandemic at a global scale, and in Brazil it's not different. The numbers of death and confirmed cases grow each day and at the same time we see opinions - from the government - trying to minimize the problem.

Motivated by this [tweet](https://twitter.com/blqueiroz/status/1253090230187548675) from Professor Bernardo Queiroz, where he states the lack of detailed data to analyze mortality ratios provided by the Ministry of Health of Brazil. This institution only provides aggregate data in a special [website](https://covid.saude.gov.br/) dedicated to Covid-19.

Other source of data is the administrative records of deaths, through death certificates. This data is provided by Brazilian Civil Registry Offices at ['Portal da Transparência - Registro Civil'](https://transparencia.registrocivil.org.br/especial-covid). This data is desaggregated by gender, age groups and brazilian states. So, we decided to collect it.

We need to point out the differences between the date:

1) more details about the data and divergences...

IMPORTANTE:

https://transparencia.registrocivil.org.br/especial-covid
"Ao selecionar o filtro tipo "Data de" com a opção óbito, sugerimos que escolha uma data de 10 dias antes da data atual. Dados mais recentes podem sofrer atualização devido a entrada de novos registros e correções de registros"

# Webscraping

So, how to collect the data if the website doesn't provide it directly via a .csv download, for exemple? We are stating here only examples of programmatic ways of getting data, because you can always copy and paste, manually.

During the [Summer School](http://evcomp.dcc.ufmg.br/) hosted by the [Computer Science Department](http://www.dcc.ufmg.br/dcc/) at UFMG, Bernardo Abreu and Vinícius Silva - from the company 'Big Data' - lectured a course on 'Collecting data from WEB'. They talked about getting data from an API and by 'Web scraping'.

The [presentation](http://evcomp.dcc.ufmg.br/wp-content/uploads/BigData-Coleta-de-dados-na-Web.pdf) has been shared and so the code - using an [API](https://colab.research.google.com/drive/1YnuhEgvSAkoonflKjM8w5-sZKJwS-mDn) or [Scraping HTML](https://colab.research.google.com/drive/1WBAyw2OQnKkrgi2xU5iT73eyW9aM_uC3).

The code and the examples shared by them helped to get what we needed to do. The data that we want to collect it's in this graph:

![Portal da Transparência](/assets/images/covid-19/graph_portal_transparencia.jpg)

A web page is constructed using the **HTML language** (and other programming languages, like CSS and JavaScript). Using the **Developer tools** in a web browser (Ctrl+Shift+I, in Google Chrome) we can look of what a web page looks like 'behind the scenes', in a [toy example](http://quotes.toscrape.com/):

{% raw %}{% include gallery caption="This is a sample gallery with **Markdown support**." %}{% endraw %}

What we have to know is that a web page is constructed by blocks and in the language the blocks have an hierarchy structure that helps us to find the information we want to collect. So, in the example above, to get a quote from Einstein we need to get the content of the tag 'span' located after several 'div' (another type of tag).

In this post, we won't go into the details of HTML or XPath, a language we can use to parse elements in XML document, and we do this because the website we're looking at, the data it's not **in** the HTML page, as you can see in the image below, so we can't scrap it right away:



Another way of getting the data it's look at One thing that we
